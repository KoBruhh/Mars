extern crate csv;
extern crate gnuplot;

use gnuplot::Figure;

// fn main() {
//     let mut x: Vec<f64> = Vec::new();
//     let mut y: Vec<f64> = Vec::new();
//     let mut fg = Figure::new();
//     let axes = fg.axes2d();
//     let mut rdr =
//         csv::Reader::from_path("/home/kobruh/github/mars/examples/example_data/test.csv").unwrap();
//     for result in rdr.records() {
//         let record = result.unwrap();
//         x.push(record[0].parse().unwrap());
//         y.push(record[1].parse().unwrap());
//         axes.points(
//             &x, // x-coordinates
//             &y, // y-coordinates
//             &[],
//         );
//     }

//     fg.show().unwrap();
// }

/* This is another section which has been created by an AI and modified by a human-like KoBruhh */

struct DataPoint {
    x: f64,
    y: f64,
}

impl DataPoint {
    fn new(x: f64, y: f64) -> Self {
        Self { x, y }
    }
}

impl Default for DataPoint {
    fn default() -> Self {
        Self { x: 0.0, y: 0.0 }
    }
}

struct Model {
    w: f64,
    b: f64,
}

impl Model {
    fn new(w: f64, b: f64) -> Self {
        Self { w, b }
    }
}

impl Default for Model {
    fn default() -> Self {
        Self { w: 0.0, b: 0.0 }
    }
}

impl Model {
    /// Returns the predicted y value for a given x value based on the model's slope and intercept.
    fn predict(&self, x: f64) -> f64 {
        self.w * x + self.b
    }

    /// Calculates the mean squared error loss for a given set of data points.
    fn loss(&self, data: &[DataPoint]) -> f64 {
        let mut loss = 0.0;
        for point in data {
            let error = self.predict(point.x) - point.y;
            loss += error * error;
        }
        loss / data.len() as f64
    }

    /// Trains the model using the gradient descent algorithm.
    fn train(&mut self, data: &[DataPoint], learning_rate: f64, num_iterations: usize) {
        for _ in 0..num_iterations {
            // Calculate the gradient of the loss function with respect to the model's parameters.
            let mut dw = 0.0;
            let mut db = 0.0;
            for point in data {
                let error = self.predict(point.x) - point.y;
                dw += error * point.x;
                db += error;
            }
            // Update the model's parameters based on the gradient and the learning rate.
            self.w -= learning_rate * dw / data.len() as f64;
            self.b -= learning_rate * db / data.len() as f64;
        }
    }
}

use std::io;

fn main() {
    let learning_rate = 0.0001;
    let num_iterations = 10000000;
    let training_data = [DataPoint::new(1.,3.), DataPoint::new(4.,6.), DataPoint::new(6.,2.)];
    let mut train_data = Vec::new();
    for i in 0..10 {
        train_data.push(DataPoint::new(i as f64, 2_f64*i as f64));
    }
    let mut model:Model = Default::default(); // Model changes it's weights and biases by the time
    // passes. That's why It has to be marked as mutable.
    model.train(&train_data, learning_rate, num_iterations);
    let loss = model.loss(&train_data);
    let pred = model.predict(10.);
    println!("Loss: {loss}, Pred 10.0: {pred}");
    loop {
        let mut input = String::new();
        io::stdin().read_line(&mut input).expect("Failed to read line");
        println!("You entered: {}", input);    }
}
